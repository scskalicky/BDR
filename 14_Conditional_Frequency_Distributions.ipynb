{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1rIqbLkLDl9_ZreESLjkHuwBGptmz-m6c",
      "authorship_tag": "ABX9TyNg6j3LHX0YAzmGqJB6UO9j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/BDR/blob/main/14_Conditional_Frequency_Distributions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k1NZY_v0Uj4"
      },
      "source": [
        "# Conditional Frequency Distributions\n",
        "\n",
        "We have explored word frequency as a variable for different texts using `nltk.FreqDist()`. In this notebook, we will explore another function called `nltk.ConditionalFreqDist()`.\n",
        "\n",
        "A Conditional Frequency Distribution extends a frequency distribution but in addition includes additional subcategories over which to form the distribution. For example, we *could* ask for a frequency distribution of modals over the entire Brown corpus, regardless of genre. This would be a simple frequency distribution.\n",
        "\n",
        "When we ask for this frequency distribution to be *dependent* upon more than one category, we are placing a *condition* on the frequency counts, making it a conditional frequency distribution.\n",
        "\n",
        "Why do we care about conditional frequency distributions? Because it is a way to directly compare different categories or subsets within a larger set of data.\n",
        "\n",
        "\n",
        "Consider the following simplistic example which compares the frequency of words between two conditions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "import nltk"
      ],
      "metadata": {
        "id": "stmn92ozeBx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQX0JOd3kVx"
      },
      "source": [
        "# Create two conditions: condition a and condition b.\n",
        "# You can see how they differ in terms of the frequency of colours in each example\n",
        "# condition a has yellow twice, and blue three times\n",
        "# condition b has one of each: yellow, blue, red\n",
        "condition_a = ['yellow', 'yellow', 'blue', 'blue', 'blue']\n",
        "condition_b = ['yellow', 'blue', 'red']\n",
        "\n",
        "# combine the conditions by concatenating the list\n",
        "combined_conditions = condition_a + condition_b\n",
        "\n",
        "# now everything is in one list\n",
        "combined_conditions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14oOyaSA5kon"
      },
      "source": [
        "If we ask for a `FreqDist()` of the combined conditions, we get an impression of frequency among these three colors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw0gb7eQ5poS"
      },
      "source": [
        "# combined frequency distribution\n",
        "fdistab = nltk.FreqDist(combined_conditions)\n",
        "fdistab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq9JW8ge5szS"
      },
      "source": [
        "However we have no way of knowing whether blue \"likes\" one condition over the other, or whether these colors occur equally among our conditions. So we could run a frequency distribution on each condition, separately:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0Hgn8Ts319N"
      },
      "source": [
        "# Frequency Distribution of condition a\n",
        "fdista = nltk.FreqDist(condition_a)\n",
        "fdista"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-IwXirr4Jnc"
      },
      "source": [
        "# Frequency distribution of condition b\n",
        "fdistb = nltk.FreqDist(condition_b)\n",
        "fdistb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSQjXlH052_l"
      },
      "source": [
        "We can compare the output and see that the two different conditions have different amounts of each color. But having to do this manually and then potentially combine the results is tiring and we would like a more efficient process.\n",
        "\n",
        "We can use the `nltk.ConditionalFreqDist()` function to do these things for us. This function will count the number of colours across both conditions. To use `ConditionalFreqDist()` (CFD), we have to provide the function with the required pieces of information:\n",
        "\n",
        "1. the conditions and\n",
        "2. the thing being counted.\n",
        "\n",
        "We then instruct the CFD function how to loop - first by condition, and then by sample (which will essentially make a nested `for loop`). Let's first transform our lists above into a dictionary structure to help clarify what is going on here. Look at the dictionary below - there are two keys (a and b), each with a list of colors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IwneE17Klao"
      },
      "source": [
        "# first combine our lists so that they are nested in a dictionary\n",
        "combined_colors = {'a': ['yellow', 'yellow', 'blue', 'blue', 'blue'],\n",
        "                   'b': ['yellow', 'blue', 'red']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HrAWzjiND1r"
      },
      "source": [
        "the `ConditionalFreqDist()` function is effectively nested loops, which can be difficult to get a grasp on at first (and probably why the authors of NLTK say to not worry about it right away). See if you can understand what the double loop is doing in the example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzoSHeQzLKTM"
      },
      "source": [
        "# first loop through the condition\n",
        "# and THEN loop through the keys of that condition, making a freq dist each time.\n",
        "\n",
        "color_cfd = nltk.ConditionalFreqDist((condition, color) # our pairs: condition = condition, color = sample\n",
        "  for condition in combined_colors # for key in dictionary, in this case a, and then b...\n",
        "  for color in combined_colors[condition]) # create freqDist of each item in the values of a and then b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the different condition\n",
        "color_cfd.conditions()"
      ],
      "metadata": {
        "id": "_PhKruO-fIrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it's helpful to know that the resulting object is fundamentally a dictionary\n",
        "color_cfd.keys()"
      ],
      "metadata": {
        "id": "TORRDA9pfLOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so we can query the conditions\n",
        "color_cfd['a']['blue']"
      ],
      "metadata": {
        "id": "qmXQLXuUfXFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the whole thing - you see they are just multiple FreqDist\n",
        "color_cfd.items()"
      ],
      "metadata": {
        "id": "S9m4jhxoQpjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNVJh4CpnqED"
      },
      "source": [
        "So, really, the conditional frequency distribution is similar to the normal frequency distribution, it just has more layers and categories.\n",
        "\n",
        "The CFD also has a few different functions. We can visualize the different counts in a matrix, which helps clarify both the presence and the absence of values in each of the two conditions. Blue clearly prefers \"a\", while \"b\" allows for all three colours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAohqjlfRPsO"
      },
      "source": [
        "# the built in tabulate method lets you make a nice table for comparison.\n",
        "color_cfd.tabulate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also make little plots, neato!\n",
        "color_cfd.plot()"
      ],
      "metadata": {
        "id": "d3EdhOODf47V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Frequency Distributions with Brown Corpus\n",
        "\n",
        "Now compare that color example to the CFD on the Brown corpus from the NLTK book. Genre is the condition (where above it was 'a' or 'b') and word frequency is the sample (where above it was the frequency of colours).\n",
        "\n",
        "We are essentially asking for the frequency of each word in `modals` conditioned by the different genres in brown."
      ],
      "metadata": {
        "id": "E0GTKxFvgBTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the required resources first.\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "1Xhp27w2gPQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGnjIhQ3Q7oU"
      },
      "source": [
        "# first create a CFD of *all* the words in Brown corpus, conditioned on genre.\n",
        "\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "           (genre, word) # condition = genre, sample = word\n",
        "           for genre in brown.categories() # for each genre in Brown\n",
        "           for word in brown.words(categories = genre)) # for each word in the genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created a conditional frequency distribution across the entire corpus, we can define more precise queries based on words and genres we are interested in."
      ],
      "metadata": {
        "id": "chsqJdKOgo_7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzGaF9z8S8YQ"
      },
      "source": [
        "# create a list of all the Brown genres\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "\n",
        "# let's again look at modal verbs only\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "\n",
        "# Ask the CFD to give us frequency of modals across all genres\n",
        "cfd.tabulate(conditions = genres, samples = modals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do these results say about word use across different genres?"
      ],
      "metadata": {
        "id": "FNYyhaXJRPX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zig9S4pm9P_8"
      },
      "source": [
        "## Comparing News and Romance\n",
        "\n",
        "Allow me to copy the Your Turn straight from the NLTK book:\n",
        "\n",
        "> *Your Turn: Working with the news and romance genres from the Brown Corpus, find out which days of the week are most newsworthy, and which are most romantic. - Define a variable called days containing a list of days of the week, i.e. `['Monday', ...]`*.\n",
        "\n",
        "> Now tabulate the counts for these words using `cfd.tabulate(samples = days)`.\n",
        "\n",
        "> Now try the same thing using plot in place of tabulate. You may control the output order of days with the help of an extra parameter: `samples = ['Monday', ...]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Lb0IqN9gp8"
      },
      "source": [
        "# first create the brown cfd\n",
        "brown_cfd = nltk.ConditionalFreqDist(\n",
        "  (genre, word)\n",
        "  for genre in brown.categories() # for each genre in the corpus\n",
        "  for word in brown.words(categories=genre)) # then for each word in the genre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1q_Ut3b9bTS"
      },
      "source": [
        "# Create the list of words you are interested in\n",
        "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "# then tabulate them\n",
        "brown_cfd.tabulate(conditions = ['romance', 'news'], samples = days, cumulative=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozWgvDPv_KEL"
      },
      "source": [
        "# plot them\n",
        "brown_cfd.plot(conditions = ['romance', 'news'], samples = days, cumulative = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqVTfsHn_qkY"
      },
      "source": [
        "What other words might make for an interesting comparison between different genres in the brown corpus? You can apply the same ideas from above to do you own investigations. Can you locate some words which clearly define different genres?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P2FPJ3TWMSU"
      },
      "source": [
        "# try looking for comparisons of different words/genres in the Brown corpus."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmRmrZb8S61H"
      },
      "source": [
        "## **Discussion**\n",
        "\n",
        "Now we are getting somewhere! We have a clear list of how modal verbs pattern across the different genres in Brown. We also have some interesting results about days of the week.\n",
        "\n",
        "Take a moment to inspect the way different genres employ modal verbs.\n",
        "\n",
        "- What can you say about romance vs news, for example?\n",
        "- Moreover, can you think of any potential mistakes we are making by comparing these direct frequency counts?\n",
        "- Any other words you would be interested in comparing?\n",
        "  - have a play with possible word targets and report your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU3QvBBSosYY"
      },
      "source": [
        "# Inaugural Address corpus\n",
        "\n",
        "Chapter 2 of NLTK includes several visualizations of the Conditional Frequency Distribution. One corpus they use is the inaugural corpus, which is a collection of speechs given by US presidents after they began a new term (so, one occurs every four years).\n",
        "\n",
        "Before looking at the CFD, load in the resource and familiarise yourself with the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in required resources\n",
        "nltk.download('inaugural')\n",
        "from nltk.corpus import inaugural"
      ],
      "metadata": {
        "id": "19kXaoRskmCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each file id contains the year and name of the US president\n",
        "inaugural.fileids()"
      ],
      "metadata": {
        "id": "PswMg77WkuAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remember, raw returns a string from the .txt file.\n",
        "# here I pick a set of words from the string to find an interesting quote.\n",
        "inaugural.raw('1945-Roosevelt.txt')[1025:1152]"
      ],
      "metadata": {
        "id": "_DHrGtGgk7j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Frequency Distribution of Inaugural\n",
        "\n",
        "Okay, the NLTK book shows how to do this, but let's repeat the process here. In the book, the authors want to examine how specific words change over time in the address. In particular, they are interested in examining how the words \"america\" and \"citizen\" change in frequency over the years. They do so using a CFD of this corpus. In this CFD, the frequency of these words are the samples, and the category or condition will be the year.  \n",
        "\n",
        "The CFD below packs a lot of information into the code.\n",
        "\n",
        "- In line 3, the first two arguments `(target, fileid[:4)` represent the word being searched and the category.\n",
        "  - the use of `fileid` means each file is considered its own category or condition\n",
        "  - slicing the fileid this way gives us the first 4 characters, which is the year\n",
        "- In line 4, a loop is then initiated over the list of filedids in the corpus.\n",
        "- In line 5, a nested loop then runs over the words of a single file, represented as `w`\n",
        "- In line 6, an additional nested loop then loops over the two target words\n",
        "- In line 7, the code askes whether `w` starts with the target. The use of `.startswith()` is a way to check if the word is the target.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hcfui932ko1k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EegImmLplP6"
      },
      "source": [
        "# CFD of Inaugural\n",
        "inaugural_cfd = nltk.ConditionalFreqDist(\n",
        "  (target, fileid[:4]) # try this with and without fileid being sliced, so you understand why they did this\n",
        "  for fileid in inaugural.fileids()\n",
        "  for w in inaugural.words(fileid)\n",
        "  for target in ['america', 'citizen']\n",
        "  if w.lower().startswith(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, the code is quite a bit to digest. Make sure you slowly go through it and understand each line. Then, inspect the results below:"
      ],
      "metadata": {
        "id": "aE45gmYHSE_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inspect the matrix of results\n",
        "inaugural_cfd.tabulate()"
      ],
      "metadata": {
        "id": "I_oVsQZ1m8FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slicing meta data from filenames\n",
        "\n",
        "How does this code figure out the year of each file? The answer is that information is included in the filename. Each filename in the Inaugural corpus is in this format:\n",
        "\n",
        "```\n",
        "YEAR-name.txt\n",
        "```\n",
        "\n",
        "Look again at line 3 in the CFD code cell above, the code asks for target and `fileid[:4]`. Slicing to 4 on fileid will return the first four characters of the filename, which happens to be the year. So this is a trick that would only work if all the filenames are standardized to follow the same format.\n",
        "\n",
        "What would happen if you just used `fileid` without slicing the year?"
      ],
      "metadata": {
        "id": "2cKV8aOwm5ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use this one simple trick to get years!\n",
        "'1945-Roosevelt.txt'[:4]"
      ],
      "metadata": {
        "id": "O7iV-3sdnfjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the CFD\n",
        "\n",
        "You can plot the frequency distribution using the built-in `plot()` method for the CFD, although I've found the plots are small and you may want to use the code below to increase the size of the plot.\n",
        "\n",
        "Examine the plot - what is it the authors of NLTK wanted you to notice about the use of `american` and `citizen` in inaugural US presidential speeches over time?\n",
        "\n",
        "\n",
        "Because the filename of each file in this corpus includes the year as the first four characters, the authors could use this as a label. There is only one speech for any year in the data because these are the speeches given by US presidents when they are elected."
      ],
      "metadata": {
        "id": "VyrtK0MvnqrA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83vm5VbtpzRG"
      },
      "source": [
        "# the plot on its own is quite small, use this code to make the plot larger\n",
        "import matplotlib.pyplot as plt\n",
        "# define the size of the figure\n",
        "plt.figure(figsize = (20, 10))\n",
        "\n",
        "# then render the plot:\n",
        "inaugural_cfd.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5fh7KF1rJ8-"
      },
      "source": [
        "The plot has frequency counts on the y-axis and year represented on the x-axis. This makes it a bit easier to compare."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn / Discussion**\n",
        "\n",
        "- What do you think of the way these terms rise and fall over the years? Do you think it can be attributed to events during that time, specifics of the speaker, or some combination of those and other factors?\n",
        "- Play around with the Inaugural Corpus - what other words might make for an interesting comparison over time?\n",
        "- What might be a problem with using raw frequency counts from these different text files?"
      ],
      "metadata": {
        "id": "qkxcDUXBuyNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Your Own Categorized Corpus\n",
        "\n",
        "In a prior notebook you were shown how to convert text files into an NLTK corpus object. Let's extend that now and use the `CategorizedPlaintextCorpusReader` to make a corpus with categories/genres.\n",
        "\n",
        "In order to do so, we need some text files, and we also need a way to indicate what genre/category we would like those files to belong to. Let's follow the NLTK authors and extract this information from the filenames.\n",
        "\n",
        "As an example, let's use some data from a [paper I published in 2015.](https://europeanjournalofhumour.org/index.php/ejhr/article/view/68)\n",
        "\n",
        "In this paper, I analysed the linguistic properties of product reviews written for the American retail website Amazon.com. I was interested in two types of reviews: legitimate review and satirical/funny reviews.\n",
        "\n",
        "The data lives here: [Amazon Data](https://github.com/scskalicky/LING-226-vuw/blob/main/other-data/amazon%20reviews.zip)\n",
        "\n",
        "Thanks to Hayden (tutor, 2023) for showing us how remarkably easy it is to use `!wget` and `unzip()` to load in a zip file and save to drive without needing to manually download and then upload. Run the code cell below to download and unzip the data into the notebook:\n",
        "\n"
      ],
      "metadata": {
        "id": "Tw11e4z6vM9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data\n",
        "!wget 'https://github.com/scskalicky/LING-226-vuw/raw/main/other-data/amazon%20reviews.zip'"
      ],
      "metadata": {
        "id": "wqzEzPqP8oDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then use `!unzip` to unzip the folder from within colab. There's an additional -d flag you can use to unzip into a directory to make working with the data easier. For example, for the amazon data we can do:\n",
        "\n",
        "\n",
        "`!unzip \"amazon reviews.zip\" -d \"amazon reviews\"`\n",
        "\n",
        "Which will give you an a folder called amazon reviews that you can use the same as if you'd mounted it from google drive, without needing to bother with files and unzipping manually.\n"
      ],
      "metadata": {
        "id": "OcbFZe209YEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"amazon reviews.zip\" -d \"amazon reviews\""
      ],
      "metadata": {
        "id": "hsfYBZLa9lwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the folder are 375 normal reviews and 375 satirical reviews.\n",
        "\n",
        "The name of each file looks like this:\n",
        "```\n",
        "001-5-satire.txt\n",
        "002-2-normal.txt\n",
        "```\n",
        "\n",
        "The first three numbers are the ID number, ranging from 1 - 375. The second number (between the two `-`) is the star rating of the review, from 1-5. The words `satire` or `normal` indicate whether the review was a normal review or a satirical funny review.\n",
        "\n",
        "We can exploit this information to make categories in our corpus. Just as the authors of NLTK sliced the year from the filename to examine change over time, we can do the same thing with these filenames to get different categories.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JnsujtP29AZ1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ct3AZQSNJvJ"
      },
      "source": [
        "# first we will load in the Corpus Reader and define the location of our texts\n",
        "import nltk\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
        "\n",
        "# set the corpus location to point to wherever it is you saved the data\n",
        "# (you may need to mount your Drive to the notebook)\n",
        "corpus_location = '/content/amazon reviews'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to use the filenames as categories, we will explot a little bit more about regular expressions (regex) patterns. What you can do now is know that we can define a pattern to capture the `normal` or `satire` portions of the filesnames using this pattern:\n",
        "\n",
        "```\n",
        ".*(......).txt\n",
        "```\n",
        "\n",
        "This pattern captures whatever is in the brackets `()`, and says give me the last six characters before `.txt` of my pattern.\n",
        "\n",
        "It corresponds to:\n",
        "\n",
        "```\n",
        "001-5-(satire).txt\n",
        "002-2-(normal).txt\n",
        "```\n",
        "\n",
        "Try it out:"
      ],
      "metadata": {
        "id": "caCO2-GD0H8j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGNNRvuFQlt_"
      },
      "source": [
        "# create a categorised corpus\n",
        "amz_corpus = CategorizedPlaintextCorpusReader(root = corpus_location, fileids = '.*', cat_pattern = '.*(......).txt')\n",
        "\n",
        "# you can check the categories\n",
        "amz_corpus.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYJT4iMSNgAd"
      },
      "source": [
        "# and we still have our fileids\n",
        "amz_corpus.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've made our corpus, we can create CFD tabulations and plots just like the NLTK book did for Brown corpus.\n",
        "\n",
        "Let's compare different words between the satirical and regular reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "RZvxSBNl1U5M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G51ezHQGSBlv"
      },
      "source": [
        "# Create a CFD of the amazon corpus\n",
        "# I am using the same code as the one for Brown with two modifications:\n",
        "# I have replaced \"genre\" with \"review_type\"\n",
        "# I lowercase the words in the corpus\n",
        "amz_cfd = nltk.ConditionalFreqDist(\n",
        "    (review_type, word)\n",
        "    for review_type in amz_corpus.categories()\n",
        "    for word in [w.lower() for w in amz_corpus.words(categories = review_type)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's ask for some specific words\n",
        "pronouns = ['i', 'me', 'you', 'my']\n",
        "\n",
        "# then tabulate them\n",
        "amz_cfd.tabulate(conditions = ['normal', 'satire'], samples = pronouns, cumulative = True)"
      ],
      "metadata": {
        "id": "jOBVORFf4GzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also plot this.\n",
        "amz_cfd.plot(conditions = ['normal', 'satire'], samples = pronouns, cumulative = True)"
      ],
      "metadata": {
        "id": "lt1Xn0c75nvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what about some other words?\n",
        "emotions = ['good', 'bad', 'happy', 'sad', 'love', 'sweet', 'hurt', 'ugly', 'nasty']\n",
        "amz_cfd.tabulate(conditions = ['normal', 'satire'], samples = emotions, cumulative = True)"
      ],
      "metadata": {
        "id": "2tUP5NRG4ygk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amz_corpus.words()"
      ],
      "metadata": {
        "id": "F3wLJhhdE3ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also wrap individual files from our corpus in `Text` so that we can look for concordances"
      ],
      "metadata": {
        "id": "XVzkqcKP5wBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the whole set of words to look at all concordances\n",
        "nltk.text.Text(amz_corpus.words()).concordance('terrible')"
      ],
      "metadata": {
        "id": "QEn5PLCS70xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also look at concordances for just one category to compare them\n",
        "# the word \"banana\" is strongly associated with the satire corpus\n",
        "nltk.text.Text(amz_corpus.words(categories = 'satire')).concordance('banana')"
      ],
      "metadata": {
        "id": "VN8Jr7hk8Un8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# but only occurs once in the non-satire corpus.\n",
        "nltk.text.Text(amz_corpus.words(categories = 'normal')).concordance('banana')"
      ],
      "metadata": {
        "id": "Hgr_RbXm8epr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "What else can you do with this corpus in terms of comparisons? You may want to scan the Skalicky and Crossley (2015) article, particularly Table 2 which lists some word categories that differed between the two review types. Think of some words that might reflect those categories - negation would include *not*, *no*, *never*, etc., whereas quantifier might include *many*, *few*, *some*, and so on. Can you find some differences in words between the two corpora using a combination of CFD and concordance lines?"
      ],
      "metadata": {
        "id": "HzJthmSSUbF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wrap Up**\n",
        "\n",
        "Being able to create your own corpus and make a comparison across categories in your corpus is a good way to develop your assessments in this course.\n",
        "\n",
        "At this point you might want to spend some time thinking about how to make your own corpus. Or, you might want to play more with this amazon review corpus. For instance - another piece of information you could pull from the corpus is the review rating which is located in the middle of the filename. The pattern to do so would be:\n",
        "\n",
        "```\n",
        ".*-(.)-.*.txt\n",
        "```\n",
        "\n",
        "Of course, you might still want to keep the satire/normal category, so perhaps expand your pattern to:\n",
        "\n",
        "```\n",
        ".*-(.-.*).txt\n",
        "```\n",
        "\n",
        "This would give you ten categories. I've typed the code below should you like to use that and do further comparisons."
      ],
      "metadata": {
        "id": "TFlvDWpK9Ad-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a categorised corpus\n",
        "amz_corpus2 = CategorizedPlaintextCorpusReader(root = corpus_location, fileids = '.*', cat_pattern = '.*-(.-.*).txt')\n",
        "\n",
        "# you can check the categories\n",
        "amz_corpus2.categories()"
      ],
      "metadata": {
        "id": "vCL3UesT9hNW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}